{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSB Assignment Report\n",
    "\n",
    "*Net Id : vvg239@nyu.edu*  \n",
    "*Name : Vaibhav Vijay Gupta*    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy that I achieved was **99.319%**. This accuracy was achieved using an **ensemble on DenseNet and MobileNet**, both trained from scratch. But to get to the final accuracy I tried out a whole lot of techniques. Below is a brief description of the techniques that I tried out along with code snippets about those. I have also included epoch vs accuracy/loss graph of the final ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1\n",
    "**(Extending the Self-Starter Code)**\n",
    "\n",
    "The starter code provided on [GitHub](https://github.com/soumith/traffic-sign-detection-homework) took my first submission directly to somewhere around 90.9%. So the challenge was to improve on this. The first approach for this was to extend the provided model. The first few submissions included the following changes:\n",
    "1. Add a layer of Conv2D layer. Adjust the number of fully connected cells accordingly. \n",
    "\n",
    "2. Add a layer of Fully Connected layer between the fc1 and fc2 layers already provided.\n",
    "\n",
    "3. Added a BatchNorm2D layer directly after conv2. Batch norm normalizes the activations (mean 0) to tackle situations like dead ReLUs. It is like normalizing the input, just instead we normalize the activations here.\n",
    "Code snippet:  \n",
    "\n",
    "```python\n",
    "    ##inside init\n",
    "    self.conv2_bn = nn.BatchNorm2d(20)\n",
    "\n",
    "    ##inside forward\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2_bn(self.conv2(x))), 2))\n",
    "```\n",
    "\n",
    "These techniques helped me get to **96.7%** accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2\n",
    "**(Using Pre-architectured Models)**\n",
    "\n",
    "After extending the self-starter model with more Conv2D, FC and BatchNorm layers, I thought it would be a good idea to try out Data Augmentation. However, the extended self-starter model gave poor results on applying RandomRotation and RandomHorizontalFlip. Thus I felt the need to train a deeper (more robust) model before trying out Data Augmentation. \n",
    "\n",
    "I tried out the following models (in order) to see which gave best results:\n",
    "1. MobileNet\n",
    "2. ResNet\n",
    "3. DenseNet\n",
    "4. VGG\n",
    "5. AlexNet\n",
    "  \n",
    "The first three gave the best results of the lot.   \n",
    "\n",
    "An important thing to note is that I used *torchvision==0.4.1*. It seems that the architecture varies from version to version, so if you try to run my code, make sure torchvision is set to the right version.  \n",
    "\n",
    "To use these pre-architectured models on GTSB, I had to move to using a custom classifier/fully-connected layer at the output end of the models. The code for ResNet is given below:  \n",
    "```python\n",
    "    model_resnet = torchvision.models.resnet18()\n",
    "\n",
    "    ## Create a custom classifer with 43(num of GTSB classes) outputs. \n",
    "    custom_fc = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(512, 200)),\n",
    "                              ('relu', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(200, 43)),\n",
    "                              ('output', nn.LogSoftmax(dim=1))\n",
    "                              ]))\n",
    "    model_resnet.fc = custom_fc\n",
    "\n",
    "```\n",
    "\n",
    "Also because these models have huge amounts of Conv layers (mostly without zero-padding), the minimum image dimensions needed are 224x224. I had to blow up the image sizes for the same. The changed data transformations looked like below:  \n",
    "```python\n",
    "    ## First resize the image to be 245x245. Then crop the 244x244 central part of image.\n",
    "    data_transforms = transforms.Compose([transforms.Resize(245),\n",
    "                                  transforms.CenterCrop(244),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))])\n",
    "```\n",
    "\n",
    "I tried a whole lot of techniques to train the model including trying out different learning rates and different optimizers like SGD, RMSProp, AdaGrad and Adam. Training Mobile Net with Adam Optimizer (learning rate 0.001 and 0.0001) got me the best performance. The prediction accuracy increased to **98.2%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 3\n",
    "**(Data Augmentation and Preprocessing)**\n",
    "\n",
    "Now that I had a sufficiently deep model it was time to again try out data augmentation. I applied `RandomRotation` and `RandomHorizontalFlip` transformations as follows:\n",
    "\n",
    "```python\n",
    "    data_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                               transforms.Resize(255),\n",
    "                               transforms.CenterCrop(244),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))])\n",
    "```\n",
    "\n",
    "Another technique that I tried applying was the **Sobel Operator**. More details about the operator [here](https://en.wikipedia.org/wiki/Sobel_operator). The code for it is below:\n",
    "```python\n",
    "    sobel_filter = torch.tensor([[-1., -2. , -1.], [0., 0., 0.], [1., 2. , 1.]])\n",
    "    ## Expand to the 3 channels for our RGB image\n",
    "    sobel_filter = sobel_filter.expand(1,3,3,3)\n",
    "    ## Apply sobel filter in both training and evaluation methods\n",
    "    images = nn.functional.conv2d(images, sobel_filter, stride=1, padding=1)\n",
    "```  \n",
    "\n",
    "Unfortunately both Data Augmentation and Sobel Operator didn't lead to any performance enhancement even after running MobileNet for around 100 epochs. This was very puzzling to me. I have a few conjectures on why this might have happened:\n",
    "1. PyTorch doesn't augment the dataset with the transformed images. Rather it does random transformations on the fly. The dataset remains of the same size N instead of becoming 3N in our case. New transformations on every epoch might be causing our model to get confused. Using `torch.concatDatasets` I could have just added fixed randomly transformed images to my dataset, but I didn't get the time to explore this.\n",
    "2. My reasoning behind using the Sobel Operator was that our signals are goemetric in shape and finding edges might help the model to perform better. However mostly the Sobel Operator is used when there is a lot of the same repeating background and our filters just end up fitting the background. This was not the case for our images. Sobel Operator might have just caused us to loose a lot of details in the model.  \n",
    "\n",
    "I am not sure about the reasons stated above and would want to explore this more (time-permitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 4\n",
    "**(Ensemble Models)**  \n",
    "\n",
    "Ensembling is the final technique that I used to go beyond 99% accuracy. As mentioned above, MobileNet and DenseNet got me the best results on the GTSB. Now was the time to apply ensembles. Ensembles are a great way to prevent overfitting while introducting more dimensionality. This is because all the individual components of an ensemble are first trained separately and then their predictions are averaged. According to Andrej Karpathy in CS231n, ensembles are a great way to improve your model accuracy by around 2%.  \n",
    "\n",
    "The backbone code for ensembling is as follows. Inside this, I plugin different ways of averaging the predictions of the individual models:\n",
    "```python\n",
    "    ## Pluging your logic to ensemble the individual models\n",
    "    class MyEnsemble(nn.Module):\n",
    "        def __init__(self, m1, m2):\n",
    "            super(MyEnsemble, self).__init__()\n",
    "            self.model1 = m1\n",
    "            self.model2 = m2\n",
    "            #Add more layers or weights or both\n",
    "\n",
    "        def forward(self, x):\n",
    "            out1 = self.model1(x)\n",
    "            out2 = self.model2(x)\n",
    "            #Add code to ensemble out1 and out2\n",
    "            return out\n",
    "```\n",
    "\n",
    "As mentioned earlier, the models are trained individually and then the averaging is done on their predictions. As a result you might want to freeze their individual parameters, as follows:\n",
    "```python\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # pass only the moving weights to the optimizer\n",
    "    optimizer = optim.Adam(model.ensemble.parameters(), lr=lr)\n",
    "```\n",
    "\n",
    "I tried out the following methods of ensembling for achieving best results:\n",
    "\n",
    "1. Add a linear layer that would take the model inputs and give the final predictions. Freeze the individual models and train only the linear layer.\n",
    "```python\n",
    "    #inside init\n",
    "    self.linear1 = nn.Linear(86, 60) #GTSB has 43 classes. So the first linear layer will have 2*43 inputs\n",
    "    self.linear2 = nn.Linear(60, 43)\n",
    "    \n",
    "    #inside forward\n",
    "    out = F.relu(self.linear1(torch.cat((out1, out2), dim=1)))\n",
    "    out = F.log_softmax(self.linear2(out), dim=1)\n",
    "    \n",
    "    model = MyEnsemble(model_resnet, model_mobilenet)\n",
    "    params = list(model.linear1.parameters()) + list(model.linear2.parameters())\n",
    "    optimizer = optim.Adam(params, lr=0.0001)\n",
    "```\n",
    "We initialize the individual models with the training checkpoints. After that you can either freeze the individual models and train only the fully connected layers or train the FC layers and the models both.  \n",
    "\n",
    "2. Use weighted averages. Different classes of the same model could be scaled using different weights.\n",
    "```python\n",
    "    #inside init\n",
    "    self.avg_weight = torch.nn.Parameter(torch.tensor([2,43], requires_grad=True))\n",
    "    \n",
    "    #inside forward\n",
    "    out = self.avg_weights[0,:]*out1 + self.weights[1,:]*out2\n",
    "    \n",
    "    model = MyEnsemble(model_dense, model_mobilenet)\n",
    "    optimizer = optim.Adam([list(model.parameters())[0]], lr=0.001)\n",
    "```  \n",
    "\n",
    "3. Use weighted averages but restrict the weights to one per model. All classes of the same model are scaled by the same weights.\n",
    "```python\n",
    "    #inside init\n",
    "    self.avg_weight = torch.nn.Parameter(torch.tensor([0.5], requires_grad=True))\n",
    "    \n",
    "    #inside forward\n",
    "    out = self.avg_weight[0]*out1 + (1-self.avg_weight[0])*out2\n",
    "    \n",
    "    model = MyEnsemble(model_dense, model_mobilenet)\n",
    "    optimizer = optim.Adam([list(model.parameters())[0]], lr=0.001)\n",
    "```  \n",
    "\n",
    "Note : The averaging weights can be treated as either learned parameters or hyper-parameters.  \n",
    "\n",
    "I tried all the above mentioned methods for different combinations of resnet, mobilenet and densenet. The best performance was achieved by taking weighted averages on the entire model outputs. I treated the weights as hyper-parameters and looped over a list of 50 values. The code is as follows:\n",
    "\n",
    "```python\n",
    "    for alpha in np.linspace(0,1,51):\n",
    "        out = alpha*out1 + (1-alpha)*out2\n",
    "```\n",
    "\n",
    "The best value of the hyper-parameter `alpha` was *0.62* and this model got me an accuracy of **99.3%**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
